{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied AI\n",
    "## Homework 1 \n",
    "### Ghazal Alinezhad Noghre \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manupilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to import libraries that we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import *\n",
    "import datetime\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[1], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, I am getting a list of files that exist in the dataset and create a pandas dataframe. Then I split them to training and validation set with the ratio of 0.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 25000\n",
      "Number of images in training set: 20000\n",
      "Number of images in validation set: 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat.797.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog.5704.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat.10234.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog.6027.jpg</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat.7413.jpg</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename label\n",
       "0    cat.797.jpg   cat\n",
       "1   dog.5704.jpg   dog\n",
       "2  cat.10234.jpg   cat\n",
       "3   dog.6027.jpg   dog\n",
       "4   cat.7413.jpg   cat"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "data_dir = \"/home/galinezh/hw/ai/hw1/train/\"\n",
    "file_names = os.listdir(data_dir)\n",
    "print('Total number of images: {}'.format(len(file_names)))\n",
    "\n",
    "files, labels = list(), list()\n",
    "for file in file_names:\n",
    "    files.append(file)\n",
    "    labels.append(file[:3])\n",
    "df = pd.DataFrame({'filename':files, 'label':labels})\n",
    "\n",
    "train_set, valid_set = train_test_split(df, test_size=0.2,random_state=seed)\n",
    "print('Number of images in training set: {}'.format(train_set.shape[0]))\n",
    "print('Number of images in validation set: {}'.format(valid_set.shape[0]))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, I normalize and augment the data using ImageDataGenerator. I use rescale for normalizing the pixel values (dividing by 255 to map the value range to 0 and 1). For augmentation I use zoom, horiontal flip, shifts and shear. keep in mind that the augmentation is only needed for training set, so I will have different generators for training and validation set. Also, I batch the data with the batch size of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 validated image filenames belonging to 2 classes.\n",
      "Found 5000 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = (150, 150)\n",
    "batch_size = 128\n",
    "\n",
    "train_datagenerator = ImageDataGenerator(rotation_range=20, \n",
    "                                  rescale=1./255, \n",
    "                                  shear_range=0.1,\n",
    "                                  zoom_range=0.1,\n",
    "                                  horizontal_flip=True,\n",
    "                                  width_shift_range=0.1,\n",
    "                                  height_shift_range=0.1)\n",
    "\n",
    "valid_datagenerator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "training_data = train_datagenerator.flow_from_dataframe(dataframe=train_set, \n",
    "                                                        directory='./train',\n",
    "                                                       x_col='filename',\n",
    "                                                       y_col='label',\n",
    "                                                       target_size=image_size,\n",
    "                                                       class_mode='categorical',\n",
    "                                                       batch_size=batch_size)\n",
    "\n",
    "validation_data = valid_datagenerator.flow_from_dataframe(dataframe=valid_set,\n",
    "                                                         directory='./train',\n",
    "                                                         x_col='filename',\n",
    "                                                         y_col='label',\n",
    "                                                         target_size=image_size,\n",
    "                                                         class_mode='categorical',\n",
    "                                                         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I use instatiate models and load them using pretrained weights. I will not use the final layers (classifier layers). Instead I add classification layer with Leaky Relu activation function and then finetune the model. The final layer has 2 neurons (representing the probablity of belonging to cat or dog class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (150, 150 ,3)\n",
    "\n",
    "VGG_model = keras.applications.vgg16.VGG16(weights=\"imagenet\", include_top=False, input_shape=input_size)\n",
    "VGG_model.trainable = False\n",
    "\n",
    "\n",
    "model_VGG = keras.models.Sequential()\n",
    "model_VGG.add(VGG_model)\n",
    "model_VGG.add(Flatten())\n",
    "model_VGG.add(Dense(128, activation=tf.nn.leaky_relu))\n",
    "model_VGG.add(Dropout(0.3))\n",
    "model_VGG.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model, I use Adam optimizer since it uses adaptive learning rate and it has shown great stability to the intial parameters. I use categorical crossentropy for loss function since we have a binary classification problem. Since I am only training the classifier (backbone is frozen), few epochs is enough.\n",
    "I am using tensorboard for logging and saving the history of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "156/156 [==============================] - 135s 863ms/step - loss: 0.7735 - accuracy: 0.8275 - val_loss: 0.2414 - val_accuracy: 0.8962\n",
      "Epoch 2/5\n",
      "156/156 [==============================] - 135s 868ms/step - loss: 0.2998 - accuracy: 0.8697 - val_loss: 0.2257 - val_accuracy: 0.9079\n",
      "Epoch 3/5\n",
      "156/156 [==============================] - 136s 874ms/step - loss: 0.2736 - accuracy: 0.8829 - val_loss: 0.2910 - val_accuracy: 0.8718\n",
      "Epoch 4/5\n",
      "156/156 [==============================] - 136s 873ms/step - loss: 0.2885 - accuracy: 0.8777 - val_loss: 0.2043 - val_accuracy: 0.9161\n",
      "Epoch 5/5\n",
      "156/156 [==============================] - 135s 867ms/step - loss: 0.3139 - accuracy: 0.8705 - val_loss: 0.2020 - val_accuracy: 0.9189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc169009908>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model_VGG.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/VGG/\" + \"VGG_transfer\"\n",
    "tensorboard_callback_VGG = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_VGG.fit(training_data,\n",
    "            epochs=5,\n",
    "            validation_data=validation_data,\n",
    "            validation_steps=valid_set.shape[0]//batch_size,\n",
    "            steps_per_epoch=train_set.shape[0]//batch_size,\n",
    "            callbacks=[tensorboard_callback_VGG] \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine tuning I unfreez the backbone, and then finetune the whole model. Also, I use a smaller learning rate since I am training the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "156/156 [==============================] - 137s 878ms/step - loss: 28.9374 - accuracy: 0.5089 - val_loss: 0.6915 - val_accuracy: 0.5016\n",
      "Epoch 2/25\n",
      "156/156 [==============================] - 138s 884ms/step - loss: 0.7749 - accuracy: 0.4988 - val_loss: 0.6962 - val_accuracy: 0.4998\n",
      "Epoch 3/25\n",
      "156/156 [==============================] - 140s 896ms/step - loss: 0.6886 - accuracy: 0.5429 - val_loss: 0.6659 - val_accuracy: 0.5917\n",
      "Epoch 4/25\n",
      "156/156 [==============================] - 139s 893ms/step - loss: 0.6796 - accuracy: 0.5587 - val_loss: 0.6413 - val_accuracy: 0.6340\n",
      "Epoch 5/25\n",
      "156/156 [==============================] - 137s 877ms/step - loss: 0.6462 - accuracy: 0.6218 - val_loss: 0.6176 - val_accuracy: 0.6815\n",
      "Epoch 6/25\n",
      "156/156 [==============================] - 139s 888ms/step - loss: 0.6073 - accuracy: 0.6658 - val_loss: 0.5601 - val_accuracy: 0.7161\n",
      "Epoch 7/25\n",
      "156/156 [==============================] - 138s 886ms/step - loss: 0.5637 - accuracy: 0.7101 - val_loss: 0.5141 - val_accuracy: 0.7446\n",
      "Epoch 8/25\n",
      "156/156 [==============================] - 138s 886ms/step - loss: 0.5436 - accuracy: 0.7223 - val_loss: 0.5075 - val_accuracy: 0.7464\n",
      "Epoch 9/25\n",
      "156/156 [==============================] - 138s 884ms/step - loss: 0.5072 - accuracy: 0.7531 - val_loss: 0.4562 - val_accuracy: 0.7833\n",
      "Epoch 10/25\n",
      "156/156 [==============================] - 138s 882ms/step - loss: 0.4873 - accuracy: 0.7668 - val_loss: 0.4327 - val_accuracy: 0.8001\n",
      "Epoch 11/25\n",
      "156/156 [==============================] - 138s 882ms/step - loss: 0.4518 - accuracy: 0.7879 - val_loss: 0.4723 - val_accuracy: 0.7885\n",
      "Epoch 12/25\n",
      "156/156 [==============================] - 138s 885ms/step - loss: 0.4256 - accuracy: 0.8069 - val_loss: 0.4051 - val_accuracy: 0.8151\n",
      "Epoch 13/25\n",
      "156/156 [==============================] - 138s 887ms/step - loss: 0.3876 - accuracy: 0.8250 - val_loss: 0.3144 - val_accuracy: 0.8698\n",
      "Epoch 14/25\n",
      "156/156 [==============================] - 138s 883ms/step - loss: 0.3371 - accuracy: 0.8510 - val_loss: 0.2855 - val_accuracy: 0.8818\n",
      "Epoch 15/25\n",
      "156/156 [==============================] - 138s 884ms/step - loss: 0.2987 - accuracy: 0.8718 - val_loss: 0.2499 - val_accuracy: 0.8958\n",
      "Epoch 16/25\n",
      "156/156 [==============================] - 138s 884ms/step - loss: 0.2716 - accuracy: 0.8852 - val_loss: 0.2162 - val_accuracy: 0.9101\n",
      "Epoch 17/25\n",
      "156/156 [==============================] - 139s 888ms/step - loss: 0.2590 - accuracy: 0.8901 - val_loss: 0.2112 - val_accuracy: 0.9083\n",
      "Epoch 18/25\n",
      "156/156 [==============================] - 138s 885ms/step - loss: 0.2223 - accuracy: 0.9044 - val_loss: 0.1933 - val_accuracy: 0.9189\n",
      "Epoch 19/25\n",
      "156/156 [==============================] - 137s 880ms/step - loss: 0.2017 - accuracy: 0.9156 - val_loss: 0.1630 - val_accuracy: 0.9287\n",
      "Epoch 20/25\n",
      "156/156 [==============================] - 139s 894ms/step - loss: 0.1833 - accuracy: 0.9240 - val_loss: 0.1765 - val_accuracy: 0.9305\n",
      "Epoch 21/25\n",
      "156/156 [==============================] - 135s 867ms/step - loss: 0.1862 - accuracy: 0.9218 - val_loss: 0.1643 - val_accuracy: 0.9377\n",
      "Epoch 22/25\n",
      "156/156 [==============================] - 138s 884ms/step - loss: 0.1726 - accuracy: 0.9273 - val_loss: 0.1423 - val_accuracy: 0.9381\n",
      "Epoch 23/25\n",
      "156/156 [==============================] - 140s 899ms/step - loss: 0.1557 - accuracy: 0.9343 - val_loss: 0.1455 - val_accuracy: 0.9359\n",
      "Epoch 24/25\n",
      "156/156 [==============================] - 139s 891ms/step - loss: 0.1467 - accuracy: 0.9409 - val_loss: 0.1428 - val_accuracy: 0.9411\n",
      "Epoch 25/25\n",
      "156/156 [==============================] - 137s 877ms/step - loss: 0.1488 - accuracy: 0.9384 - val_loss: 0.1394 - val_accuracy: 0.9407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc1690095f8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_VGG.layers[0].trainable=True\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_VGG.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/VGG/\" + \"VGG_finetune\"\n",
    "tensorboard_callback_VGG = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_VGG.fit(training_data,\n",
    "            epochs=25,\n",
    "            validation_data=validation_data,\n",
    "            validation_steps=valid_set.shape[0]//batch_size,\n",
    "            steps_per_epoch=train_set.shape[0]//batch_size,\n",
    "            callbacks=[tensorboard_callback_VGG] \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "input_size = (150, 150 ,3)\n",
    "\n",
    "MobileNetV2_model = keras.applications.MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_size)\n",
    "MobileNetV2_model.trainable = False\n",
    "\n",
    "\n",
    "model_MobileNetV2 = keras.models.Sequential()\n",
    "model_MobileNetV2.add(MobileNetV2_model)\n",
    "model_MobileNetV2.add(Flatten())\n",
    "model_MobileNetV2.add(Dense(128, activation=tf.nn.leaky_relu))\n",
    "model_MobileNetV2.add(Dropout(0.3))\n",
    "model_MobileNetV2.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model, I use Adam optimizer since it uses adaptive learning rate and it has shown great stability to the intial parameters. I use categorical crossentropy for loss function since we have a binary classification problem. Since I am only training the classifier (backbone is frozen), few epochs is enough.\n",
    "I am using tensorboard for logging and saving the history of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "156/156 [==============================] - 135s 866ms/step - loss: 12.7261 - accuracy: 0.9054 - val_loss: 0.5631 - val_accuracy: 0.9621\n",
      "Epoch 2/5\n",
      "156/156 [==============================] - 133s 855ms/step - loss: 1.2644 - accuracy: 0.9285 - val_loss: 1.2925 - val_accuracy: 0.9631\n",
      "Epoch 3/5\n",
      "156/156 [==============================] - 133s 851ms/step - loss: 0.9516 - accuracy: 0.9305 - val_loss: 1.0261 - val_accuracy: 0.9555\n",
      "Epoch 4/5\n",
      "156/156 [==============================] - 133s 850ms/step - loss: 0.7977 - accuracy: 0.9349 - val_loss: 0.2714 - val_accuracy: 0.9473\n",
      "Epoch 5/5\n",
      "156/156 [==============================] - 133s 853ms/step - loss: 0.6150 - accuracy: 0.9355 - val_loss: 0.4389 - val_accuracy: 0.9653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3fdcfc2ac8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model_MobileNetV2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/MobileNetV2/\" + \"MobileNetV2_transfer\"\n",
    "tensorboard_callback_MobileNetV2 = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_MobileNetV2.fit(training_data,\n",
    "            epochs=5,\n",
    "            validation_data=validation_data,\n",
    "            validation_steps=valid_set.shape[0]//batch_size,\n",
    "            steps_per_epoch=train_set.shape[0]//batch_size,\n",
    "            callbacks=[tensorboard_callback_MobileNetV2] \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine tuning I unfreez the backbone, and then finetune the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "156/156 [==============================] - 137s 879ms/step - loss: 0.2693 - accuracy: 0.9256 - val_loss: 46.9865 - val_accuracy: 0.5319\n",
      "Epoch 2/25\n",
      "156/156 [==============================] - 137s 878ms/step - loss: 0.0968 - accuracy: 0.9632 - val_loss: 13.1798 - val_accuracy: 0.6759\n",
      "Epoch 3/25\n",
      "156/156 [==============================] - 135s 867ms/step - loss: 0.0656 - accuracy: 0.9749 - val_loss: 3.1204 - val_accuracy: 0.8662\n",
      "Epoch 4/25\n",
      "156/156 [==============================] - 134s 862ms/step - loss: 0.0649 - accuracy: 0.9756 - val_loss: 2.2655 - val_accuracy: 0.8914\n",
      "Epoch 5/25\n",
      "156/156 [==============================] - 135s 868ms/step - loss: 0.0589 - accuracy: 0.9782 - val_loss: 0.4449 - val_accuracy: 0.9451\n",
      "Epoch 6/25\n",
      "156/156 [==============================] - 135s 867ms/step - loss: 0.0480 - accuracy: 0.9806 - val_loss: 0.3923 - val_accuracy: 0.9609\n",
      "Epoch 7/25\n",
      "156/156 [==============================] - 135s 866ms/step - loss: 0.0453 - accuracy: 0.9843 - val_loss: 0.5124 - val_accuracy: 0.9497\n",
      "Epoch 8/25\n",
      "156/156 [==============================] - 136s 870ms/step - loss: 0.0444 - accuracy: 0.9825 - val_loss: 0.2320 - val_accuracy: 0.9615\n",
      "Epoch 9/25\n",
      "156/156 [==============================] - 136s 869ms/step - loss: 0.0449 - accuracy: 0.9843 - val_loss: 0.2502 - val_accuracy: 0.9625\n",
      "Epoch 10/25\n",
      "156/156 [==============================] - 135s 864ms/step - loss: 0.0368 - accuracy: 0.9866 - val_loss: 0.4797 - val_accuracy: 0.9471\n",
      "Epoch 11/25\n",
      "156/156 [==============================] - 135s 864ms/step - loss: 0.0439 - accuracy: 0.9842 - val_loss: 0.2329 - val_accuracy: 0.9589\n",
      "Epoch 12/25\n",
      "156/156 [==============================] - 135s 866ms/step - loss: 0.0422 - accuracy: 0.9847 - val_loss: 0.5838 - val_accuracy: 0.9409\n",
      "Epoch 13/25\n",
      "156/156 [==============================] - 135s 867ms/step - loss: 0.0349 - accuracy: 0.9865 - val_loss: 0.3591 - val_accuracy: 0.9499\n",
      "Epoch 14/25\n",
      "156/156 [==============================] - 134s 862ms/step - loss: 0.0404 - accuracy: 0.9852 - val_loss: 0.6185 - val_accuracy: 0.9325\n",
      "Epoch 15/25\n",
      "156/156 [==============================] - 135s 862ms/step - loss: 0.0417 - accuracy: 0.9847 - val_loss: 10.1175 - val_accuracy: 0.7194\n",
      "Epoch 16/25\n",
      "156/156 [==============================] - 135s 864ms/step - loss: 0.0414 - accuracy: 0.9854 - val_loss: 0.3289 - val_accuracy: 0.9459\n",
      "Epoch 17/25\n",
      "156/156 [==============================] - 134s 862ms/step - loss: 0.0344 - accuracy: 0.9874 - val_loss: 0.4722 - val_accuracy: 0.9523\n",
      "Epoch 18/25\n",
      "156/156 [==============================] - 134s 861ms/step - loss: 0.0287 - accuracy: 0.9898 - val_loss: 0.5370 - val_accuracy: 0.9253\n",
      "Epoch 19/25\n",
      "156/156 [==============================] - 134s 862ms/step - loss: 0.0530 - accuracy: 0.9807 - val_loss: 1.8580 - val_accuracy: 0.8962\n",
      "Epoch 20/25\n",
      "156/156 [==============================] - 135s 867ms/step - loss: 0.5917 - accuracy: 0.8467 - val_loss: 6.2373 - val_accuracy: 0.5002\n",
      "Epoch 21/25\n",
      "156/156 [==============================] - 136s 869ms/step - loss: 0.4557 - accuracy: 0.8023 - val_loss: 9.5688 - val_accuracy: 0.5000\n",
      "Epoch 22/25\n",
      "156/156 [==============================] - 135s 864ms/step - loss: 0.2671 - accuracy: 0.8915 - val_loss: 4.6884 - val_accuracy: 0.4992\n",
      "Epoch 23/25\n",
      "156/156 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9223"
     ]
    }
   ],
   "source": [
    "model_MobileNetV2.layers[0].trainable=True\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_MobileNetV2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/MobileNetV2/\" + \"MobileNetV2_finetune\"\n",
    "tensorboard_callback_MobileNetV2 = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_MobileNetV2.fit(training_data,\n",
    "            epochs=25,\n",
    "            validation_data=validation_data,\n",
    "            validation_steps=valid_set.shape[0]//batch_size,\n",
    "            steps_per_epoch=train_set.shape[0]//batch_size,\n",
    "            callbacks=[tensorboard_callback_MobileNetV2] \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (150, 150 ,3)\n",
    "\n",
    "ResNet50_model = keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_size)\n",
    "ResNet50_model.trainable = False\n",
    "\n",
    "\n",
    "model_ResNet50 = keras.models.Sequential()\n",
    "model_ResNet50.add(ResNet50_model)\n",
    "model_ResNet50.add(Flatten())\n",
    "model_ResNet50.add(Dense(128, activation=tf.nn.leaky_relu))\n",
    "model_ResNet50.add(Dropout(0.3))\n",
    "model_ResNet50.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model, I use Adam optimizer since it uses adaptive learning rate and it has shown great stability to the intial parameters. I use categorical crossentropy for loss function since we have a binary classification problem. Since I am only training the classifier (backbone is frozen), few epochs is enough.\n",
    "I am using tensorboard for logging and saving the history of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "156/156 [==============================] - 138s 885ms/step - loss: 4.9825 - accuracy: 0.5224 - val_loss: 0.8811 - val_accuracy: 0.5030\n",
      "Epoch 2/5\n",
      "156/156 [==============================] - 140s 895ms/step - loss: 0.8481 - accuracy: 0.5532 - val_loss: 0.6743 - val_accuracy: 0.5829\n",
      "Epoch 3/5\n",
      "156/156 [==============================] - 139s 889ms/step - loss: 0.7032 - accuracy: 0.5844 - val_loss: 0.6717 - val_accuracy: 0.5719\n",
      "Epoch 4/5\n",
      "156/156 [==============================] - 136s 869ms/step - loss: 0.6911 - accuracy: 0.5899 - val_loss: 0.6425 - val_accuracy: 0.6334\n",
      "Epoch 5/5\n",
      "156/156 [==============================] - 135s 866ms/step - loss: 0.6688 - accuracy: 0.6081 - val_loss: 0.6968 - val_accuracy: 0.5335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0d98c66160>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model_ResNet50.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/ResNet50/\" + \"ResNet50_transfer\"\n",
    "tensorboard_callback_ResNet50 = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_ResNet50.fit(training_data,\n",
    "            epochs=5,\n",
    "            validation_data=validation_data,\n",
    "            validation_steps=valid_set.shape[0]//batch_size,\n",
    "            steps_per_epoch=train_set.shape[0]//batch_size,\n",
    "            callbacks=[tensorboard_callback_ResNet50] \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine tuning I unfreez the backbone, and then finetune the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/156 [..............................] - ETA: 56s - loss: 12.7468 - accuracy: 0.5078WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.249277). Check your callbacks.\n",
      "156/156 [==============================] - 138s 882ms/step - loss: 0.6316 - accuracy: 0.8102 - val_loss: 0.7199 - val_accuracy: 0.5000\n",
      "Epoch 2/30\n",
      "156/156 [==============================] - 136s 872ms/step - loss: 0.1531 - accuracy: 0.9375 - val_loss: 0.9190 - val_accuracy: 0.5004\n",
      "Epoch 3/30\n",
      "156/156 [==============================] - 135s 864ms/step - loss: 0.1127 - accuracy: 0.9541 - val_loss: 0.7215 - val_accuracy: 0.5000\n",
      "Epoch 4/30\n",
      "156/156 [==============================] - 136s 875ms/step - loss: 0.0965 - accuracy: 0.9622 - val_loss: 0.7080 - val_accuracy: 0.5038\n",
      "Epoch 5/30\n",
      "156/156 [==============================] - 137s 877ms/step - loss: 0.0856 - accuracy: 0.9677 - val_loss: 0.6612 - val_accuracy: 0.6300\n",
      "Epoch 6/30\n",
      "156/156 [==============================] - 136s 873ms/step - loss: 0.0854 - accuracy: 0.9670 - val_loss: 0.4891 - val_accuracy: 0.7706\n",
      "Epoch 7/30\n",
      "156/156 [==============================] - 137s 877ms/step - loss: 0.0788 - accuracy: 0.9703 - val_loss: 0.1313 - val_accuracy: 0.9569\n",
      "Epoch 8/30\n",
      "156/156 [==============================] - 137s 877ms/step - loss: 0.0748 - accuracy: 0.9713 - val_loss: 0.1357 - val_accuracy: 0.9493\n",
      "Epoch 9/30\n",
      "156/156 [==============================] - 137s 875ms/step - loss: 0.0733 - accuracy: 0.9713 - val_loss: 0.3217 - val_accuracy: 0.8996\n",
      "Epoch 10/30\n",
      "156/156 [==============================] - 137s 878ms/step - loss: 0.0695 - accuracy: 0.9727 - val_loss: 0.6093 - val_accuracy: 0.7927\n",
      "Epoch 11/30\n",
      "156/156 [==============================] - 137s 877ms/step - loss: 0.0748 - accuracy: 0.9711 - val_loss: 0.2050 - val_accuracy: 0.9379\n",
      "Epoch 12/30\n",
      "156/156 [==============================] - 137s 876ms/step - loss: 0.0672 - accuracy: 0.9742 - val_loss: 0.1247 - val_accuracy: 0.9567\n",
      "Epoch 13/30\n",
      "156/156 [==============================] - 138s 884ms/step - loss: 0.0716 - accuracy: 0.9741 - val_loss: 0.1978 - val_accuracy: 0.9129\n",
      "Epoch 14/30\n",
      "156/156 [==============================] - 136s 875ms/step - loss: 0.0651 - accuracy: 0.9760 - val_loss: 0.1947 - val_accuracy: 0.9393\n",
      "Epoch 15/30\n",
      "156/156 [==============================] - 137s 877ms/step - loss: 0.0647 - accuracy: 0.9740 - val_loss: 0.3120 - val_accuracy: 0.8698\n",
      "Epoch 16/30\n",
      "156/156 [==============================] - 137s 878ms/step - loss: 0.0631 - accuracy: 0.9757 - val_loss: 0.9244 - val_accuracy: 0.8099\n",
      "Epoch 17/30\n",
      "156/156 [==============================] - 136s 875ms/step - loss: 0.0626 - accuracy: 0.9767 - val_loss: 0.3090 - val_accuracy: 0.9111\n",
      "Epoch 18/30\n",
      "156/156 [==============================] - 134s 858ms/step - loss: 0.0565 - accuracy: 0.9788 - val_loss: 0.2047 - val_accuracy: 0.9319\n",
      "Epoch 19/30\n",
      "156/156 [==============================] - 134s 858ms/step - loss: 0.0701 - accuracy: 0.9739 - val_loss: 0.2341 - val_accuracy: 0.8856\n",
      "Epoch 20/30\n",
      "156/156 [==============================] - 134s 862ms/step - loss: 0.0701 - accuracy: 0.9734 - val_loss: 0.1830 - val_accuracy: 0.9311\n",
      "Epoch 21/30\n",
      "156/156 [==============================] - 134s 861ms/step - loss: 0.0545 - accuracy: 0.9791 - val_loss: 0.1854 - val_accuracy: 0.9443\n",
      "Epoch 22/30\n",
      "156/156 [==============================] - 135s 865ms/step - loss: 0.0595 - accuracy: 0.9776 - val_loss: 0.2152 - val_accuracy: 0.9073\n",
      "Epoch 23/30\n",
      "156/156 [==============================] - 135s 868ms/step - loss: 0.0617 - accuracy: 0.9760 - val_loss: 0.1574 - val_accuracy: 0.9403\n",
      "Epoch 24/30\n",
      "156/156 [==============================] - 135s 867ms/step - loss: 0.0585 - accuracy: 0.9786 - val_loss: 0.1825 - val_accuracy: 0.9263\n",
      "Epoch 25/30\n",
      "156/156 [==============================] - 135s 865ms/step - loss: 0.0486 - accuracy: 0.9808 - val_loss: 0.1501 - val_accuracy: 0.9427\n",
      "Epoch 26/30\n",
      "156/156 [==============================] - 135s 864ms/step - loss: 0.0551 - accuracy: 0.9787 - val_loss: 0.1522 - val_accuracy: 0.9489\n",
      "Epoch 27/30\n",
      "156/156 [==============================] - 135s 863ms/step - loss: 0.0521 - accuracy: 0.9807 - val_loss: 0.2345 - val_accuracy: 0.9189\n",
      "Epoch 28/30\n",
      "156/156 [==============================] - 135s 866ms/step - loss: 0.0532 - accuracy: 0.9808 - val_loss: 1.0472 - val_accuracy: 0.7450\n",
      "Epoch 29/30\n",
      "156/156 [==============================] - 135s 868ms/step - loss: 0.0597 - accuracy: 0.9772 - val_loss: 0.2339 - val_accuracy: 0.9277\n",
      "Epoch 30/30\n",
      "156/156 [==============================] - 135s 867ms/step - loss: 0.0512 - accuracy: 0.9816 - val_loss: 0.2890 - val_accuracy: 0.8834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0d90170fd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ResNet50.layers[0].trainable=True\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_ResNet50.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/ResNet50/\" + \"ResNet50_finetune\"\n",
    "tensorboard_callback_ResNet50 = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_ResNet50.fit(training_data,\n",
    "            epochs=30,\n",
    "            validation_data=validation_data,\n",
    "            validation_steps=valid_set.shape[0]//batch_size,\n",
    "            steps_per_epoch=train_set.shape[0]//batch_size,\n",
    "            callbacks=[tensorboard_callback_ResNet50] \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined three functions. Augmentation is for randomly applying transforms to the input image. It uses 5 different transforms such as random brightness, flip, hue and contrast. Also, a very important step in preprocessing is normalization. Normalization should be applied to both train and validation set. In normalization function I simply resize the image to the desired size and then divide value of each pixel to 255 to mapp the range of pizel value to 0 and 1. Finally, I have a load_data function that loads, augments, normilizes and splits the input data. I only apply augmentation to train set as it helps the model to be more robust to noises and smal variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augmentation (image):\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_hue(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, 0.1, 0.25)\n",
    "    return image\n",
    "\n",
    "def normalization (image):\n",
    "    image = tf.image.resize(image, [150, 150])\n",
    "    image = tf.cast(image, tf.float16)\n",
    "    image = (image / 255.0)\n",
    "    return image\n",
    "\n",
    "def load_data(data_dir, split, augment=False, augment_factor=2):\n",
    "    print('Total number of images: {}'.format(len(os.listdir(data_dir))))\n",
    "\n",
    "    files, labels = list(), list()\n",
    "    for file in os.listdir(data_dir):\n",
    "        files.append(file)\n",
    "        labels.append(file[:3])\n",
    "    df = pd.DataFrame({'filename':files, 'label':labels})\n",
    "\n",
    "    train_set, valid_set = train_test_split(df, test_size=split,random_state=seed)\n",
    "    print('Number of images in training set: {}'.format(train_set.shape[0]))\n",
    "    print('Number of images in validation set: {}'.format(valid_set.shape[0]))\n",
    "\n",
    "    images_valid = []\n",
    "    labels_valid = []\n",
    "    # pbar = tqdm(total=len(valid_set))\n",
    "    # pbar.set_description(\"Loading Validation Set\")\n",
    "    for index, row in valid_set.iterrows():\n",
    "        img = cv2.imread(os.path.join(data_dir,row['filename']))\n",
    "        if img is not None:\n",
    "            images_valid.append(normalization(img))\n",
    "            if row['label'] == 'cat':\n",
    "                labels_valid.append(0)\n",
    "            else:\n",
    "                labels_valid.append(1)\n",
    "            # pbar.update()\n",
    "\n",
    "    images_train = []\n",
    "    labels_train = []\n",
    "    pbar2 = tqdm(total=len(train_set))\n",
    "    pbar2.set_description(\"Loading Train Set\")\n",
    "    for index, row in train_set.iterrows():\n",
    "        img = cv2.imread(os.path.join(data_dir,row['filename']))\n",
    "        if img is not None:\n",
    "            images_train.append(normalization(img))\n",
    "            if row['label'] == 'cat':\n",
    "                labels_train.append(0)\n",
    "            else:\n",
    "                labels_train.append(1)\n",
    "            pbar2.update()\n",
    "            if augment:\n",
    "                for i in range (augment_factor):\n",
    "                    image = augmentation (normalization(img))\n",
    "                    images_train.append(image)\n",
    "                    if row['label'] == 'cat':\n",
    "                        labels_train.append(0)\n",
    "                    else:\n",
    "                        labels_train.append(1)\n",
    "\n",
    "    return images_train, labels_train, images_valid, labels_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 25000\n",
      "Number of images in training set: 20000\n",
      "Number of images in validation set: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Train Set: 100%|█████████▉| 19995/20000 [04:34<00:00, 73.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Train Set: 100%|██████████| 20000/20000 [04:50<00:00, 73.62it/s]"
     ]
    }
   ],
   "source": [
    "images_train, labels_train, images_valid, labels_valid = load_data (data_dir, 0.2, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train_array = np.array(images_train)\n",
    "labels_train_array = np.array(labels_train)\n",
    "images_valid_array = np.array(images_valid)\n",
    "labels_valid_array = np.array(labels_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (150, 150 ,3)\n",
    "\n",
    "VGG_model = keras.applications.vgg16.VGG16(weights=\"imagenet\", include_top=False, input_shape=input_size)\n",
    "VGG_model.trainable = False\n",
    "\n",
    "\n",
    "model_VGG = keras.models.Sequential()\n",
    "model_VGG.add(VGG_model)\n",
    "model_VGG.add(Flatten())\n",
    "model_VGG.add(Dense(128, activation=tf.nn.leaky_relu))\n",
    "model_VGG.add(Dropout(0.3))\n",
    "model_VGG.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model, I use Adam optimizer since it uses adaptive learning rate and it has shown great stability to the intial parameters. I use categorical crossentropy for loss function since we have a binary classification problem. Since I am only training the classifier (backbone is frozen), few epochs is enough.\n",
    "I am using tensorboard for logging and saving the history of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 47s 38ms/step - loss: 0.4431 - accuracy: 0.7724 - val_loss: 0.2233 - val_accuracy: 0.9046\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 0.3749 - accuracy: 0.8156 - val_loss: 0.2038 - val_accuracy: 0.9132\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 0.3528 - accuracy: 0.8306 - val_loss: 0.2084 - val_accuracy: 0.9164\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 0.3408 - accuracy: 0.8364 - val_loss: 0.2040 - val_accuracy: 0.9170\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 0.3250 - accuracy: 0.8443 - val_loss: 0.2395 - val_accuracy: 0.9060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f79797ba1d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_VGG.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/VGG/\" + \"VGG_transfer_manual\"\n",
    "tensorboard_callback_VGG = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_VGG.fit(images_train_array,\n",
    "            labels_train_array,\n",
    "            batch_size=32,\n",
    "            epochs=5,\n",
    "            validation_data=(images_valid_array, labels_valid_array),\n",
    "            callbacks=[tensorboard_callback_VGG] \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine tuning I unfreez the backbone, and then finetune the whole model. Also, I use a smaller learning rate since I am training the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1250/1250 [==============================] - 110s 88ms/step - loss: 9.1429 - accuracy: 0.5045 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 2/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 3/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6931 - accuracy: 0.5061 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 4/30\n",
      "1250/1250 [==============================] - 110s 88ms/step - loss: 0.6932 - accuracy: 0.5037 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 5/30\n",
      "1250/1250 [==============================] - 109s 88ms/step - loss: 0.6933 - accuracy: 0.4978 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 6/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6933 - accuracy: 0.4987 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 7/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6932 - accuracy: 0.5017 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 8/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6932 - accuracy: 0.4990 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 9/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6933 - accuracy: 0.4984 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 10/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6933 - accuracy: 0.4974 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 11/30\n",
      "1250/1250 [==============================] - 110s 88ms/step - loss: 0.6933 - accuracy: 0.4963 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 12/30\n",
      "1250/1250 [==============================] - 109s 88ms/step - loss: 0.6933 - accuracy: 0.4983 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 13/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6932 - accuracy: 0.5040 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 14/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6932 - accuracy: 0.4994 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 15/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 16/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6933 - accuracy: 0.4978 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 17/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6932 - accuracy: 0.5020 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 18/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 19/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6934 - accuracy: 0.4965 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 20/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6933 - accuracy: 0.5033 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 21/30\n",
      "1250/1250 [==============================] - 110s 88ms/step - loss: 0.6933 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 22/30\n",
      "1250/1250 [==============================] - 110s 88ms/step - loss: 0.6934 - accuracy: 0.4957 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 23/30\n",
      "1250/1250 [==============================] - 110s 88ms/step - loss: 0.6933 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 24/30\n",
      "1250/1250 [==============================] - 109s 88ms/step - loss: 0.6933 - accuracy: 0.4967 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 25/30\n",
      "1250/1250 [==============================] - 109s 87ms/step - loss: 0.6934 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 26/30\n",
      "1250/1250 [==============================] - 109s 88ms/step - loss: 0.6933 - accuracy: 0.5029 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 27/30\n",
      "1250/1250 [==============================] - 109s 88ms/step - loss: 0.6934 - accuracy: 0.4958 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 28/30\n",
      "1250/1250 [==============================] - 110s 88ms/step - loss: 0.6933 - accuracy: 0.4978 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 29/30\n",
      "1250/1250 [==============================] - 110s 88ms/step - loss: 0.6933 - accuracy: 0.4971 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 30/30\n",
      "1250/1250 [==============================] - 110s 88ms/step - loss: 0.6933 - accuracy: 0.5011 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f797957a6a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_VGG.layers[0].trainable=True\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0015)\n",
    "model_VGG.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/VGG/\" + \"VGG_finetune_manual\"\n",
    "tensorboard_callback_VGG = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_VGG.fit(images_train_array,\n",
    "            labels_train_array,\n",
    "            batch_size=32,\n",
    "            epochs=30,\n",
    "            validation_data=(images_valid_array, labels_valid_array),\n",
    "            callbacks=[tensorboard_callback_VGG] \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "input_size = (150, 150 ,3)\n",
    "\n",
    "MobileNetV2_model = keras.applications.MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=input_size)\n",
    "MobileNetV2_model.trainable = False\n",
    "\n",
    "\n",
    "model_MobileNetV2 = keras.models.Sequential()\n",
    "model_MobileNetV2.add(MobileNetV2_model)\n",
    "model_MobileNetV2.add(Flatten())\n",
    "model_MobileNetV2.add(Dense(128, activation=tf.nn.leaky_relu))\n",
    "model_MobileNetV2.add(Dropout(0.3))\n",
    "model_MobileNetV2.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model, I use Adam optimizer since it uses adaptive learning rate and it has shown great stability to the intial parameters. I use categorical crossentropy for loss function since we have a binary classification problem. Since I am only training the classifier (backbone is frozen), few epochs is enough.\n",
    "I am using tensorboard for logging and saving the history of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 6.8427 - accuracy: 0.8399 - val_loss: 3.5835 - val_accuracy: 0.9570\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 9.2395 - accuracy: 0.8715 - val_loss: 7.5227 - val_accuracy: 0.9478\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 11.7363 - accuracy: 0.8831 - val_loss: 8.9021 - val_accuracy: 0.9538\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 11.6314 - accuracy: 0.8931 - val_loss: 6.5515 - val_accuracy: 0.9488\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 20s 16ms/step - loss: 12.6740 - accuracy: 0.9004 - val_loss: 15.2347 - val_accuracy: 0.9446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8db0bfa160>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model_MobileNetV2.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/MobileNetV2/\" + \"MobileNetV2_transfer_manual\"\n",
    "tensorboard_callback_MobileNetV2 = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_MobileNetV2.fit(images_train_array,\n",
    "            labels_train_array,\n",
    "            batch_size=32,\n",
    "            epochs=5,\n",
    "            validation_data=(images_valid_array, labels_valid_array),\n",
    "            callbacks=[tensorboard_callback_MobileNetV2] \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine tuning I unfreez the backbone, and then finetune the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "   2/1250 [..............................] - ETA: 4:07 - loss: 59.1360 - accuracy: 0.7812WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.169261). Check your callbacks.\n",
      "1250/1250 [==============================] - 86s 69ms/step - loss: 1.9769 - accuracy: 0.8581 - val_loss: 12.1659 - val_accuracy: 0.8718\n",
      "Epoch 2/25\n",
      "1250/1250 [==============================] - 90s 72ms/step - loss: 0.2022 - accuracy: 0.9275 - val_loss: 0.5182 - val_accuracy: 0.9478\n",
      "Epoch 3/25\n",
      "1250/1250 [==============================] - 89s 71ms/step - loss: 0.8656 - accuracy: 0.8098 - val_loss: 2.2088 - val_accuracy: 0.7562\n",
      "Epoch 4/25\n",
      "1250/1250 [==============================] - 88s 71ms/step - loss: 0.2304 - accuracy: 0.9103 - val_loss: 0.2171 - val_accuracy: 0.9294\n",
      "Epoch 5/25\n",
      "1250/1250 [==============================] - 88s 71ms/step - loss: 0.1364 - accuracy: 0.9457 - val_loss: 0.1458 - val_accuracy: 0.9484\n",
      "Epoch 6/25\n",
      "1250/1250 [==============================] - 89s 71ms/step - loss: 0.0902 - accuracy: 0.9651 - val_loss: 0.1484 - val_accuracy: 0.9534\n",
      "Epoch 7/25\n",
      "1250/1250 [==============================] - 89s 71ms/step - loss: 0.0740 - accuracy: 0.9717 - val_loss: 0.2288 - val_accuracy: 0.9496\n",
      "Epoch 8/25\n",
      "1250/1250 [==============================] - 89s 71ms/step - loss: 0.0926 - accuracy: 0.9652 - val_loss: 2.8949 - val_accuracy: 0.9302\n",
      "Epoch 9/25\n",
      "1250/1250 [==============================] - 89s 72ms/step - loss: 0.0771 - accuracy: 0.9699 - val_loss: 0.9973 - val_accuracy: 0.9208\n",
      "Epoch 10/25\n",
      "1250/1250 [==============================] - 89s 71ms/step - loss: 0.0710 - accuracy: 0.9729 - val_loss: 0.5843 - val_accuracy: 0.9522\n",
      "Epoch 11/25\n",
      "1250/1250 [==============================] - 95s 76ms/step - loss: 0.0652 - accuracy: 0.9754 - val_loss: 2.4987 - val_accuracy: 0.9006\n",
      "Epoch 12/25\n",
      "1250/1250 [==============================] - 89s 71ms/step - loss: 1.4791 - accuracy: 0.5918 - val_loss: 18.6992 - val_accuracy: 0.5920\n",
      "Epoch 13/25\n",
      "1250/1250 [==============================] - 89s 71ms/step - loss: 0.8702 - accuracy: 0.5042 - val_loss: 0.7758 - val_accuracy: 0.4914\n",
      "Epoch 14/25\n",
      "1250/1250 [==============================] - 89s 71ms/step - loss: 0.8197 - accuracy: 0.5010 - val_loss: 0.7357 - val_accuracy: 0.4986\n",
      "Epoch 15/25\n",
      "1250/1250 [==============================] - 88s 71ms/step - loss: 0.8288 - accuracy: 0.5356 - val_loss: 1.5520 - val_accuracy: 0.5240\n",
      "Epoch 16/25\n",
      "1250/1250 [==============================] - 88s 71ms/step - loss: 0.7921 - accuracy: 0.5502 - val_loss: 1.5263 - val_accuracy: 0.4752\n",
      "Epoch 17/25\n",
      "1250/1250 [==============================] - 88s 71ms/step - loss: 0.8707 - accuracy: 0.5153 - val_loss: 1.3574 - val_accuracy: 0.4994\n",
      "Epoch 18/25\n",
      "1250/1250 [==============================] - 88s 71ms/step - loss: 0.7540 - accuracy: 0.5055 - val_loss: 0.9731 - val_accuracy: 0.5352\n",
      "Epoch 19/25\n",
      "1250/1250 [==============================] - 91s 73ms/step - loss: 0.7993 - accuracy: 0.4994 - val_loss: 0.7702 - val_accuracy: 0.5166\n",
      "Epoch 20/25\n",
      "1250/1250 [==============================] - 89s 71ms/step - loss: 0.7137 - accuracy: 0.5004 - val_loss: 0.7074 - val_accuracy: 0.5000\n",
      "Epoch 21/25\n",
      "1250/1250 [==============================] - 89s 71ms/step - loss: 0.7054 - accuracy: 0.5063 - val_loss: 0.7311 - val_accuracy: 0.5592\n",
      "Epoch 22/25\n",
      "1250/1250 [==============================] - 88s 71ms/step - loss: 0.7006 - accuracy: 0.5167 - val_loss: 0.6947 - val_accuracy: 0.5000\n",
      "Epoch 23/25\n",
      "1250/1250 [==============================] - 88s 71ms/step - loss: 0.8814 - accuracy: 0.5003 - val_loss: 0.7412 - val_accuracy: 0.5092\n",
      "Epoch 24/25\n",
      " 369/1250 [=======>......................] - ETA: 59s - loss: 0.7139 - accuracy: 0.4989"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_MobileNetV2.layers[0].trainable=True\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_MobileNetV2.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/MobileNetV2/\" + \"MobileNetV2_finetune_manual\"\n",
    "tensorboard_callback_MobileNetV2 = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_MobileNetV2.fit(images_train_array,\n",
    "            labels_train_array,\n",
    "            batch_size=32,\n",
    "            epochs=25,\n",
    "            validation_data=(images_valid_array, labels_valid_array),\n",
    "            callbacks=[tensorboard_callback_MobileNetV2] \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (150, 150 ,3)\n",
    "\n",
    "ResNet50_model = keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_size)\n",
    "ResNet50_model.trainable = False\n",
    "\n",
    "\n",
    "model_ResNet50 = keras.models.Sequential()\n",
    "model_ResNet50.add(ResNet50_model)\n",
    "model_ResNet50.add(Flatten())\n",
    "model_ResNet50.add(Dense(128, activation=tf.nn.leaky_relu))\n",
    "model_ResNet50.add(Dropout(0.3))\n",
    "model_ResNet50.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model, I use Adam optimizer since it uses adaptive learning rate and it has shown great stability to the intial parameters. I use categorical crossentropy for loss function since we have a binary classification problem. Since I am only training the classifier (backbone is frozen), few epochs is enough.\n",
    "I am using tensorboard for logging and saving the history of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 40s 32ms/step - loss: 1.5925 - accuracy: 0.5210 - val_loss: 1.2900 - val_accuracy: 0.5016\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 39s 31ms/step - loss: 0.9396 - accuracy: 0.5408 - val_loss: 1.0348 - val_accuracy: 0.5308\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 1.1573 - accuracy: 0.5423 - val_loss: 0.8156 - val_accuracy: 0.5288\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 38s 31ms/step - loss: 1.0921 - accuracy: 0.5592 - val_loss: 0.7789 - val_accuracy: 0.5928\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 1.4550 - accuracy: 0.5550 - val_loss: 0.5754 - val_accuracy: 0.7088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f77042c40f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model_ResNet50.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/ResNet50/\" + \"ResNet50_transfer_manual\"\n",
    "tensorboard_callback_ResNet50 = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_ResNet50.fit(images_train_array,\n",
    "            labels_train_array,\n",
    "            batch_size=32,\n",
    "            epochs=5,\n",
    "            validation_data=(images_valid_array, labels_valid_array),\n",
    "            callbacks=[tensorboard_callback_ResNet50] \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine tuning I unfreez the backbone, and then finetune the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "   2/1250 [..............................] - ETA: 6:41 - loss: 23.1868 - accuracy: 0.4844WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.288877). Check your callbacks.\n",
      "1250/1250 [==============================] - 102s 81ms/step - loss: 0.4609 - accuracy: 0.8325 - val_loss: 0.2045 - val_accuracy: 0.9166\n",
      "Epoch 2/25\n",
      "1250/1250 [==============================] - 100s 80ms/step - loss: 0.1999 - accuracy: 0.9213 - val_loss: 0.3725 - val_accuracy: 0.8610\n",
      "Epoch 3/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.1443 - accuracy: 0.9424 - val_loss: 3.7035 - val_accuracy: 0.5824\n",
      "Epoch 4/25\n",
      "1250/1250 [==============================] - 100s 80ms/step - loss: 1.0788 - accuracy: 0.5871 - val_loss: 0.6403 - val_accuracy: 0.6404\n",
      "Epoch 5/25\n",
      "1250/1250 [==============================] - 98s 79ms/step - loss: 0.6737 - accuracy: 0.5934 - val_loss: 0.6036 - val_accuracy: 0.6802\n",
      "Epoch 6/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.6311 - accuracy: 0.6409 - val_loss: 0.5232 - val_accuracy: 0.7358\n",
      "Epoch 7/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.5674 - accuracy: 0.7028 - val_loss: 0.4938 - val_accuracy: 0.7774\n",
      "Epoch 8/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.4625 - accuracy: 0.7837 - val_loss: 0.6223 - val_accuracy: 0.6622\n",
      "Epoch 9/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.3946 - accuracy: 0.8141 - val_loss: 0.2985 - val_accuracy: 0.8868\n",
      "Epoch 10/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.2596 - accuracy: 0.8902 - val_loss: 0.2491 - val_accuracy: 0.8876\n",
      "Epoch 11/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.1893 - accuracy: 0.9239 - val_loss: 0.3082 - val_accuracy: 0.8736\n",
      "Epoch 12/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.1482 - accuracy: 0.9404 - val_loss: 0.1716 - val_accuracy: 0.9254\n",
      "Epoch 13/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.1294 - accuracy: 0.9495 - val_loss: 0.1719 - val_accuracy: 0.9382\n",
      "Epoch 14/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.7042 - accuracy: 0.6233 - val_loss: 0.5816 - val_accuracy: 0.6978\n",
      "Epoch 15/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.4542 - accuracy: 0.7626 - val_loss: 0.2298 - val_accuracy: 0.9084\n",
      "Epoch 16/25\n",
      "1250/1250 [==============================] - 100s 80ms/step - loss: 0.1557 - accuracy: 0.9367 - val_loss: 0.5954 - val_accuracy: 0.7686\n",
      "Epoch 17/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.1861 - accuracy: 0.9291 - val_loss: 0.1835 - val_accuracy: 0.9360\n",
      "Epoch 18/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.0934 - accuracy: 0.9627 - val_loss: 0.4187 - val_accuracy: 0.8790\n",
      "Epoch 19/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.0738 - accuracy: 0.9713 - val_loss: 0.3003 - val_accuracy: 0.9148\n",
      "Epoch 20/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.2882 - accuracy: 0.8755 - val_loss: 0.5505 - val_accuracy: 0.7208\n",
      "Epoch 21/25\n",
      "1250/1250 [==============================] - 99s 80ms/step - loss: 0.3232 - accuracy: 0.8436 - val_loss: 0.1917 - val_accuracy: 0.9274\n",
      "Epoch 22/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.0746 - accuracy: 0.9721 - val_loss: 0.2000 - val_accuracy: 0.9310\n",
      "Epoch 23/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.0471 - accuracy: 0.9823 - val_loss: 0.2675 - val_accuracy: 0.9350\n",
      "Epoch 24/25\n",
      "1250/1250 [==============================] - 100s 80ms/step - loss: 0.0481 - accuracy: 0.9823 - val_loss: 0.2646 - val_accuracy: 0.9238\n",
      "Epoch 25/25\n",
      "1250/1250 [==============================] - 99s 79ms/step - loss: 0.1757 - accuracy: 0.9387 - val_loss: 0.2070 - val_accuracy: 0.9268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f76fc1c98d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ResNet50.layers[0].trainable=True\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_ResNet50.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/ResNet50/\" + \"ResNet50_finetune_manual\"\n",
    "tensorboard_callback_ResNet50 = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model_ResNet50.fit(images_train_array,\n",
    "            labels_train_array,\n",
    "            batch_size=32,\n",
    "            epochs=25,\n",
    "            validation_data=(images_valid_array, labels_valid_array),\n",
    "            callbacks=[tensorboard_callback_ResNet50] \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the results and training process logs are saveed in log directory and using Tensorboard you can check them out. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
